{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "230f7f13-ddec-4dec-b674-64a8f8473b03",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dc3159a-cc1e-4fd3-90ef-eb0322504046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import json\n",
    "from transformers import DistilBertTokenizerFast, TFDistilBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0c9116-a70d-4247-9bdb-2bea0968d92d",
   "metadata": {},
   "source": [
    "### Load Diesis Mapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "351b9efd-9701-425f-a116-ee744f538132",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./label_map.json', 'r') as f:\n",
    "        reverse_label_map = json.load(f)\n",
    "    \n",
    "# Reverse the mapping to ID -> Disease Name\n",
    "label_map = {v: k for k, v in reverse_label_map.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a66da4-518d-4409-8bd5-db08a7258782",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2f7fc147-6bbe-4810-a252-fda96ca4b437",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 14 variables whereas the saved optimizer has 26 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RestoredOptimizer` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.RestoredOptimizer`.\n"
     ]
    }
   ],
   "source": [
    "x_ray_model_fracture=load_model('./x_ray/bon_fracture.keras')\n",
    "\n",
    "x_ray_model_lung=load_model('./x_ray/bon_fracture.keras')\n",
    "\n",
    "symptom2dieses_model= tf.saved_model.load('./Symptom2Dieses/sympomt2Dieses')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208a87bb-d24c-458f-8daf-d41d4ebc405b",
   "metadata": {},
   "source": [
    "### Initialize Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "03f58ed0-a63a-4dd4-b189-a817a90c84f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2741f6-83b0-472e-af43-aa4f5a47d8fc",
   "metadata": {},
   "source": [
    "### Fracture Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "09335430-d812-46b8-b303-6b2878c37e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Fractured\n",
      "Fracture Prediction: [[0.6960164]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load and preprocess X-ray image\n",
    "def preprocess_xray_image(img_path, target_size=(180, 180)):  # Assuming your model input size is 224x224\n",
    "    img = image.load_img(img_path, target_size=target_size, color_mode='rgb')  # Use 'grayscale' if needed\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    img_array = img_array / 255.0  # Normalize pixel values (if required by the model)\n",
    "    return img_array\n",
    "\n",
    "# Preprocess the image\n",
    "xray_image_path = './fracture.jpeg'\n",
    "xray_input = preprocess_xray_image(xray_image_path)\n",
    "\n",
    "# Predict using the X-ray model\n",
    "fracture_prediction = x_ray_model_fracture.predict(xray_input)\n",
    "threshold = 0.5\n",
    "\n",
    "# Convert prediction to class label\n",
    "if fracture_prediction[0][0] > threshold:\n",
    "    result = \"Fractured\"\n",
    "else:\n",
    "    result = \"Not Fractured\"\n",
    "print(result)\n",
    "print(\"Fracture Prediction:\", fracture_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac01bcc-2d6f-482f-a705-c0ef7b56b80a",
   "metadata": {},
   "source": [
    "### Lung Diesis Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5c9fe181-85cd-45bb-a200-2653f5fa4c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Fracture Prediction: COVID19\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load and preprocess X-ray image\n",
    "def preprocess_xray_image(img_path, target_size=(180, 180)):\n",
    "    img = image.load_img(img_path, target_size=target_size, color_mode='rgb')  # Assuming RGB input\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    img_array = img_array / 255.0  # Normalize pixel values\n",
    "    return img_array\n",
    "\n",
    "# Preprocess the image\n",
    "xray_image_path = './Chest.jpg'\n",
    "xray_input = preprocess_xray_image(xray_image_path)\n",
    "\n",
    "# Predict using the X-ray model\n",
    "lung_prediction = x_ray_model_lung.predict(xray_input)\n",
    "\n",
    "# Get predicted class index (highest probability)\n",
    "predicted_class_index = np.argmax(lung_prediction, axis=-1)\n",
    "\n",
    "# Define class labels\n",
    "class_labels = ['COVID19', 'NORMAL', 'PNEUMONIA', 'TURBERCULOSIS']  # Adjust class labels accordingly\n",
    "\n",
    "# Get predicted class label\n",
    "predicted_class_label = class_labels[predicted_class_index[0]]\n",
    "\n",
    "print(\"Fracture Prediction:\", predicted_class_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0e608a-2197-4f82-95d4-b9985b174417",
   "metadata": {},
   "source": [
    "### Symptom to Diesis Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1da6f95e-4423-4d3e-886c-a3c4948a0268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import keras\\nimport numpy as np'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class index: 236\n",
      "Predicted class label: Common Cold\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_test=[\"\"\"I've been feeling really tired, and I keep getting chills. \n",
    "        My throat hurts, and I can’t stop coughing. \n",
    "        It feels like I have a fever, and my muscles ache all over.\"\"\"]\n",
    "# Example input data (make sure the shape matches the model's input shape)\n",
    "test_encodings = tokenizer(\n",
    "    x_test,\n",
    "    return_tensors=\"tf\",         # Return TensorFlow tensors\n",
    "    truncation=True,             # Enable truncation of sequences\n",
    "    padding=\"max_length\",        # Pad sequences to the maximum length\n",
    "    max_length=512               # Set the maximum length to 512 tokens\n",
    ")\n",
    "input_encodings = tokenizer(x_test, truncation=True, padding=True, return_tensors='tf')\n",
    "\n",
    "predictions = model(input_encodings)\n",
    "logits = predictions['logits'] \n",
    "probabilities = tf.nn.softmax(logits)\n",
    "predicted_class_index = np.argmax(probabilities.numpy())\n",
    "print(\"Predicted class index:\", predicted_class_index)\n",
    "predicted_class_label = label_map[predicted_class_index]\n",
    "print(\"Predicted class label:\", predicted_class_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
